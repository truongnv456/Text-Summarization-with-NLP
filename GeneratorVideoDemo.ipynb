{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","collapsed_sections":["R_vD29rsqZ6x"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ngockhanh5110/nlp-vietnamese-text-summarization/blob/main/notebooks/inferencing_vietnews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["**Overview: The code below is for the purpose of comparing the quality of the algorithms Lsa, KL, LexRank, TextRank, SumBasic, Luhn with the model I trained**"],"metadata":{"id":"B0Yrht9UMVa1"}},{"cell_type":"markdown","source":["# **Connect to drive**"],"metadata":{"id":"KmpYKfiC4kty"}},{"cell_type":"code","metadata":{"id":"A6hRf-gcm92k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692030494205,"user_tz":-420,"elapsed":20589,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"7a75dfef-8229-44da-a27f-a7f87569c9ab"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# **Install lib**"],"metadata":{"id":"R_vD29rsqZ6x"}},{"cell_type":"code","source":["!pip install vncorenlp datasets==1.0.2\n","!pip install transformers\n","!pip install dill==0.3.5.1\n","!pip install torch\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vlpu5EVqhdw","executionInfo":{"status":"ok","timestamp":1692030532306,"user_tz":-420,"elapsed":32137,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"2ec92141-792d-4d0f-bce4-09bd4fe62c94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vncorenlp\n","  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/2.6 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets==1.0.2\n","  Downloading datasets-1.0.2-py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==1.0.2) (1.23.5)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.0.2) (9.0.0)\n","Collecting dill (from datasets==1.0.2)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.0.2) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.0.2) (2.31.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from datasets==1.0.2) (4.66.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==1.0.2) (3.12.2)\n","Collecting xxhash (from datasets==1.0.2)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.0.2) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.0.2) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==1.0.2) (1.16.0)\n","Building wheels for collected packages: vncorenlp\n","  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645933 sha256=21bf8bfe6b5b71e5c6de575b97d20588176eda7db5e7d098a30ef5b26ce786dc\n","  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n","Successfully built vncorenlp\n","Installing collected packages: xxhash, dill, vncorenlp, datasets\n","Successfully installed datasets-1.0.2 dill-0.3.7 vncorenlp-1.0.3 xxhash-3.3.0\n","Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Collecting dill==0.3.5.1\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dill\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.7\n","    Uninstalling dill-0.3.7:\n","      Successfully uninstalled dill-0.3.7\n","Successfully installed dill-0.3.5.1\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install git-python==1.0.3\n","# !pip install sacrebleu==1.4.12\n","!pip install rouge_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8NqBV0aq7rI","executionInfo":{"status":"ok","timestamp":1692030552648,"user_tz":-420,"elapsed":20359,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"917bf975-c04e-4230-b297-cdf41525585c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git-python==1.0.3\n","  Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n","Collecting gitpython (from git-python==1.0.3)\n","  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython->git-python==1.0.3)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python==1.0.3)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, gitdb, gitpython, git-python\n","Successfully installed git-python-1.0.3 gitdb-4.0.10 gitpython-3.1.32 smmap-5.0.0\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=fe291713a9372e30bd5bbbecafbfd6618d813773f10763d06dfa5b3683375825\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}]},{"cell_type":"code","metadata":{"id":"Gx4Vg4cbEUJ_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692030552649,"user_tz":-420,"elapsed":19,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"1757f3bf-2c52-430c-bdc1-323148cc8141"},"source":["import os\n","new_path = \"/content/drive/MyDrive/LLM_TEXT/\"\n","os.chdir(new_path)\n","current_path = os.getcwd()\n","print(\"current path:\", current_path)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["current path: /content/drive/MyDrive/LLM_TEXT\n"]}]},{"cell_type":"code","metadata":{"id":"TZYjioRkKviO","executionInfo":{"status":"ok","timestamp":1692030570534,"user_tz":-420,"elapsed":17898,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b6e675f-9445-427d-d4bf-e1fc84085666"},"source":["import glob\n","import pandas as pd\n","import concurrent.futures\n","from datasets import *\n","import datasets\n","import transformers\n","import nltk\n","nltk.download('punkt')\n","\n","from transformers import RobertaTokenizerFast,AutoTokenizer\n","from seq2seq_trainer import Seq2SeqTrainer\n","from transformers import TrainingArguments\n","from dataclasses import dataclass, field\n","from typing import Optional"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["!pip install sumy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAIj5bRP4hS7","executionInfo":{"status":"ok","timestamp":1692030619553,"user_tz":-420,"elapsed":49060,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"5c87f44e-2548-4106-8b0f-0949b2666b08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sumy\n","  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m865.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting breadability>=0.1.20 (from sumy)\n","  Downloading breadability-0.1.20.tar.gz (32 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n","Collecting pycountry>=18.2.23 (from sumy)\n","  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n","Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2023.7.22)\n","Building wheels for collected packages: breadability, docopt, pycountry\n","  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=42eb4ea4b8ddc6322f66dbf4ff9ac8b072680c80abce1993777ecf604f2324df\n","  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=8fb4c18f040ce52bc1c2cebe4271ed4f2be169c8c2d9da2831545bfcc939c3b2\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681831 sha256=a6f2b2139231c093f55460b0db3aafecfb95d97428d665720a118af6dcfe8b40\n","  Stored in directory: /root/.cache/pip/wheels/03/57/cc/290c5252ec97a6d78d36479a3c5e5ecc76318afcb241ad9dbe\n","Successfully built breadability docopt pycountry\n","Installing collected packages: docopt, pycountry, breadability, sumy\n","Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"]}]},{"cell_type":"code","source":["!pip install dateparser\n","!pip install datefinder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6QwNJUqDCq8","executionInfo":{"status":"ok","timestamp":1692030638089,"user_tz":-420,"elapsed":18540,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"23c9e5b2-fe17-4d14-f87a-d876f025c202"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dateparser\n","  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser) (2.8.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser) (2023.3)\n","Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2023.6.3)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser) (5.0.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser) (1.16.0)\n","Installing collected packages: dateparser\n","Successfully installed dateparser-1.1.8\n","Collecting datefinder\n","  Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.10/dist-packages (from datefinder) (2023.6.3)\n","Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from datefinder) (2.8.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datefinder) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.16.0)\n","Installing collected packages: datefinder\n","Successfully installed datefinder-0.7.3\n"]}]},{"cell_type":"code","source":["listFile = [\"07.txt\", \"08.txt\", \"09.txt\", \"10.txt\", \"11.txt\", \"12.txt\", \"13.txt\"]"],"metadata":{"id":"uxpC8HDS5GNp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4IteMtlc58-y"},"source":["# **Processing data**"]},{"cell_type":"code","source":["size_data_test = 50\n","summary_task_original_new = []"],"metadata":{"id":"2BIOn5u5CVfp"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVkc5HmK6Bdd"},"source":["# Đọc danh sách các file từ đường dẫn được cung cấp\n","def listPaths(path):\n","  pathfiles = list()\n","  cnt = 0\n","  for pathfile in glob.glob(path):\n","    cnt += 1\n","    file_name = os.path.basename(pathfile)\n","    for file_ in  listFile:\n","      if file_name == file_:\n","        pathfiles.append(pathfile)\n","        break\n","    if cnt == size_data_test:\n","       break\n","  return pathfiles\n","\n","test_paths = listPaths('/content/drive/MyDrive/LLM_TEXT/dataset/BBCHnew/*.txt')\n","# for file in test_paths:\n","#   print(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Đọc nội dung file\n","def read_content(path_file):\n","  \"\"\"\n","  Input: Path of txt file\n","  Output\n","  \"\"\"\n","  # print(\"\\n\\n path_file:\", path_file)\n","  summary_content = \"\"\n","  summary_task = \"\"\n","  original = \"\"\n","  summary_taskDict = []\n","  with open(path_file, \"r\", encoding=\"utf-8\") as file:\n","      lines  = file.readlines()\n","      cnt = 0\n","      # print(\"\\n\\n path_file:\",path_file)\n","      # print(\"\\n\\n lines:\",lines)\n","      for line in lines:\n","          line = line.strip()\n","\n","          if \"NOI_DUNG_TOM_TAT\" in line:\n","              cnt = 1\n","              line = line.replace(\"NOI_DUNG_TOM_TAT\", \"\")\n","              line = line.replace(\":\", \"\")\n","          elif \"NHIEM_VU\" in line:#line.startswith(\"NHIEM_VU\"):\n","              cnt = 2\n","              line = line.replace(\"NHIEM_VU\", \"\")\n","              line = line.replace(\":\", \"\")\n","          # new code\n","          elif \"NGUOI_LAM\" in line: #line.startswith(\"NGUOI_LAM\"):\n","              summary_taskDict.append({ \"people\": \"\", \"content\": \"\", \"time\": \"\"})\n","              cnt = 20\n","              line = line.replace(\"NGUOI_LAM\", \"\")\n","              line = line.replace(\":\", \"\")\n","          elif \"NOI_DUNG_CV\" in line: #line.startswith(\"NOI_DUNG_CV\"):\n","              cnt = 21\n","              line = line.replace(\"NOI_DUNG_CV\", \"\")\n","              line = line.replace(\":\", \"\")\n","          elif \"THOI_GIAN\" in line: #line.startswith(\"THOI_GIAN\"):\n","              cnt = 22\n","              line = line.replace(\"THOI_GIAN\", \"\")\n","              line = line.replace(\":\", \"\")\n","          #end\n","          elif \"NOI_DUNG_GOC\" in line: #line.startswith(\"NOI_DUNG_GOC\"):\n","              cnt = 3\n","              line = line.replace(\"NOI_DUNG_GOC\", \"\")\n","              line = line.replace(\":\", \"\")\n","\n","          line += \" \"\n","          # line  = line.replace(\"..\", \". \")\n","          if line:\n","            if cnt == 1:\n","              summary_content += line\n","            #elif cnt == 2:\n","            #  summary_task += line\n","            elif cnt == 20:\n","              summary_taskDict[-1][\"people\"] += line\n","            elif cnt == 21:\n","              summary_taskDict[-1][\"content\"] += line\n","            elif cnt == 22:\n","              summary_taskDict[-1][\"time\"] += line\n","\n","            elif cnt == 3:\n","              original += line\n","\n","  summary_task_original_new.append({'file' : path_file,\n","          'summary_task': summary_taskDict}) # chứa các nội dung people: tên người làm, time: thời gian, và content: nội dung nhiệm vụ\n","\n","  return {'file' : path_file,\n","          # 'summary_task': summary_taskDict,\n","          'summary_content': summary_content, # chứa nội dung tóm tắt cuộc họp\n","          'original': original} # văn bản cuộc họp gốc ( văn bản cần tóm tắt (trích xuất))"],"metadata":{"id":"p7SudQRE8w3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Đọc các key word\n","def get_stop_words_vn(path):\n","    ret = []\n","    # path = \"/content/drive/MyDrive/LLM_TEXT/vietnamese-stopwords-dash.txt\"\n","    f = open(path, \"r\")\n","    lines = f.readlines()\n","    for line  in lines:\n","       line = line.replace(\"_\", \" \")\n","       line = \" \" + line.replace(\"\\n\", \"\") + \" \"\n","       ret.append(line)\n","    return ret\n","\n","stop_words = get_stop_words_vn(\"/content/drive/MyDrive/LLM_TEXT/vietnamese-stopwords-dash.txt\") # chứa các keyword cho việc xóa các từ dừng trong tiếng việt\n","task_words = get_stop_words_vn(\"/content/drive/MyDrive/LLM_TEXT/vietnamese-task-dash.txt\") # chứa các keyword dành cho trích xuất nhiệm vụ"],"metadata":{"id":"pO1jXQg02ikj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Get data**"],"metadata":{"id":"r3sWCYVKHXRe"}},{"cell_type":"code","source":["def get_dataframe(pathfiles):\n","  with concurrent.futures.ProcessPoolExecutor() as executor:\n","    data = executor.map(read_content, pathfiles)\n","\n","  # Make blank dataframe\n","  data_df = list()\n","  for d in data:\n","    data_df.append(d)\n","  data_df = pd.DataFrame(data_df)\n","  data_df.dropna(inplace = True)\n","  data_df = data_df.sample(frac=1).reset_index(drop=True)\n","\n","  return data_df\n","\n","test_df = get_dataframe(test_paths)\n","summary_task_original_new = []\n","for path_file in test_df[\"file\"]:\n","  read_content(path_file)"],"metadata":{"id":"9DrQo8ysHWGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Summary task**"],"metadata":{"id":"jDoq6yHWIGrw"}},{"cell_type":"code","source":["import nltk\n","from nltk import sent_tokenize\n","# nltk.download('punkt')\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# đọc model PhoBERT cho việc trích xuất nhiệm vụ\n","tokenizer_nvu = AutoTokenizer.from_pretrained('vinai/phobert-base')\n","model_nvu = AutoModel.from_pretrained('vinai/phobert-base')\n","\n","# chức năng chia nhỏ đoạn văn thành các đoạn nhỏ sử dụng text segmentation in NLP\n","def content_sentences( body: str, min_length=5, max_length=200):\n","        sents = [s.strip() for s in sent_tokenize(body)]\n","        return [c for c in sents if max_length > len(c.split(\" \")) > min_length]\n","\n","# Sử dụng PhoBERT để convert text thành các dạng vector\n","def encode_sentences(sentences):\n","    encoded_inputs = tokenizer_nvu(sentences, padding=True, truncation=True, return_tensors='pt')\n","    with torch.no_grad():\n","        model_output = model_nvu(**encoded_inputs)\n","    return model_output.pooler_output\n","\n","# Đánh giá sự tương đồng của 2 đoạn (về nội dung, ngữ nghĩa) => trả về giá trị 0 - 1.0\n","def calculate_cosine_similarity(sentence_infer_embedding, sentence2):\n","    sentence2_embedding = encode_sentences(sentence2)\n","    similarity_score = cosine_similarity(sentence_infer_embedding, sentence2_embedding)\n","    return similarity_score[0][0]\n","\n","\n","\n","# Ví dụ:\n","sentence1 = \"Quỳnh nhận nhiệm vụ sẽ tuyên truyền giáo dục An toàn giao thông cho học sinh vào các giờ chào cờ đầu tuần.\"\n","sentence2 = \"Trường THCS kim Liên diễn ra cuộc họp của ban trung tâm bàn nội dung công việc và nhiệm vụ thực hiện của tháng 5..\"\n","\n","sentence_infer_embedding = encode_sentences(sentence1)\n","similarity_percentage = calculate_cosine_similarity(sentence_infer_embedding, sentence2)\n","print(f\"Độ giống nhau: {similarity_percentage:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypaOKDdFatH0","executionInfo":{"status":"ok","timestamp":1692036761687,"user_tz":-420,"elapsed":2919,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"d1b1f869-644d-4425-cee1-c79373db1272"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["Độ giống nhau: 0.47%\n"]}]},{"cell_type":"code","source":["inferrence_Task_embedding = [] # Tạo list data để làm dữ liệu tham chiếu cho việc trích xuất nhiệm vụ, chứa các mã encoder cho việc trích xuất nhiệm vụ\n","\n","for listT in summary_task_original_new:\n","    for content in listT['summary_task']:\n","      content_ = content['content']\n","      sentences = content_sentences(content_, 1, 400)\n","      for sentence in sentences:\n","         print(\"sentence \", sentence )\n","         encode_sentence_ = encode_sentences(sentence)\n","         inferrence_Task_embedding.append(encode_sentence_)\n","         break\n","\n","print(\"length of inferrence_Task_embedding: \", len(inferrence_Task_embedding))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIj6ur-6cdG7","executionInfo":{"status":"ok","timestamp":1692036773689,"user_tz":-420,"elapsed":9713,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"9887b6b5-6490-4fe7-9cf2-ad4e48a9bfef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sentence  Lan chủ tịch công đoàn là chủ tọa  điều hành cuộc họp và phân công nhiệm vụ cụ thể  cho các thành viên.\n","sentence  Hương đảm nhận  kiểm tra, giám sát việc thực hiện điều lệ, kế hoạch, nghị quyết của CĐCS.\n","sentence  Mạnh đảm nhận xây dựng kế hoạch cho ngày kỉ niệm 76 Quốc khánh nước cộng hoà xã hội chủ nghĩa Việt Nam\n","sentence  Kiên đảm nhận kết hợp với các đồng chí Lan, và Hương thực hiện tốt nhiệm vụ được phân công\n","sentence  Ông Tùng là tổ trưởng chuyên môn  điều hành cuộc họp và  phân công nhiệm vụ cho các thành viên trong tổ nghiên cứu, thảo luận và đánh giá  theo tiêu chí  các văn bản hướng dẫn sau Thông tư số 31/2021/TT-BGDĐT ngày 23/08/2021 của Bộ Giáo dục và Đào tạo Quy định việc lựa chọn sách giáo khoa trong cơ sở giáo dục phổ thông.\n","sentence  Huệ  đảm nhận tìm hiểu về cấu trúc các bài trong một chương và mối liên hệ giữa các bài học.\n","sentence  Hạnh đảm nhận tìm hiểu nội dung hướng nghiệp.\n","sentence  đảm nhận nội dung giáo dục An toàn giao thông\n","sentence  Linh sẽ đảm nhận trách nhiệm hướng dẫn CBGVNV cùng nắm chắc các phương pháp phòng chống bệnh viêm đường hô hấp cấp do chủng mới của vi rút Corona đặc biệt chú trọng công tác an toàn thực phẩm, kiểm tra vệ sinh, khử khuẩn phòng chống dịch tại trường.\n","sentence  Hùng đảm nhận phụ trách công tác y tế, tham mưu, giúp đỡ và cùng phối hợp với nhà trường trong công tác phòng chống bệnh viêm đường hô hấp cấp do chủng mới của vi rút Corona tổ chức các buổi tuyên truyền về công tác phòng chống bệnh.\n","sentence  Hạnh phối kết hợp cùng các đồng chí trong Ban chỉ đạo theo dõi, giám sát việc thực hiện công tác bệnh viêm đường hô hấp cấp do chủng mới của vi rút Corona.\n","sentence  Nguyên đảm nhận tham gia kiểm tra, theo dõi, giám sát việc thực hiện công tác phòng chống bệnh viêm đường hô hấp cấp do chủng mới của vi rút Corona\n","sentence  Trang đảm nhận tiếp nhận, làm hồ sơ nhập viện cho bệnh nhân mới.\n","sentence  Hùng đảm nhận  trực và điều trị  các ca bệnh cấp cứu, các bệnh nhân nặng.\n","sentence  Hải tiếp tục điều trị bệnh nhân ở các phòng theo phác đồ đã điều trị.\n","sentence  Nga đảm nhận làm thủ tục xuất viện cho bệnh nhân đã khỏi bệnh.\n","sentence  Quyết sẽ đảm nhận nhiệm vụ tạo bản vẽ kiến trúc tổng quan của chung cư, bao gồm Thiết kế bố trí các tòa nhà, khu vực công cộng và các tiện ích khác.\n","sentence  Linh đảm nhận nhiệm vụ thiết kế nội thất cho các căn hộ trong chung cư, bao gồm Lựa chọn các vật liệu, màu sắc và phong cách thiết kế phù hợp.\n","sentence  Chi đảm nhận nhiệm vụ thiết kế khu vực xanh và không gian ngoại thất của chung cư, bao gồm Tạo ra các khu vườn, sân thượng và khu vực thư giãn cho cư dân.\n","sentence  Dũng đảm nhận nhiệm vụ thiết kế hệ thống kỹ thuật và an ninh của chung cư, bao gồm Thiết kế hệ thống điện, hệ thống cấp thoát nước và hệ thống điều hòa không khí.\n","sentence  Ông Lợi bí thư chi bộ là người điều hành thông báo quyết định của đảng bộ bà phân công nhiệm vụ cho từng thành viên.\n","sentence  Lương sẽ đảm nhận trực tiếp phụ trách các lĩnh vực công tác xây dựng Đảng, chế độ thông tin, báo cáo và bảo mật, tham mưu cho chi ủy triển khai thực hiện Quy chế dân chủ.\n","sentence  Đậm phụ trách các lĩnh vực công tác Kiểm tra, giám sát và thi hành kỷ luật của Chi bộ, Chi ủy; công tác giải quyết khiếu nại, tố cáo; phụ trách chỉ đạo hoạt động của các đoàn thể.\n","sentence  Hương đảm nhận phụ trách các lĩnh vực công tác Dân vận, báo cáo viên, tuyên truyền viên, công tác phát triển đảng viên của Chi bộ.\n","sentence  Chung thông báo nội dung công việc mà tổng công ty ban hành và giao nhiệm vụ cho từng thành viên.\n","sentence  Thanh đảm nhận giữ chức vụ phó giám đốc nhiệm vụ của ông là giúp giám đốc  chỉ đạo và chịu trách nhiệm theo quy định của pháp luật về các lĩnh vực bảo trì, sửa chữa đường bộ, quy hoạch sử dụng đất các công trình, công tác chuẩn bị đầu tư và quản lý nhà nước các dự án đầu tư bằng nguồn vốn ODA.\n","sentence  Thắng đảm nhận giữ chức vụ phó giám đốc có nhiệm vụ giúp Giám đốc chỉ đạo và chịu trách nhiệm theo quy định của pháp luật đối với lĩnh vực quản lý nhà nước về hoạt động vận tải, công nghiệp, trực tiếp chỉ đạo, theo dõi các đơn vị thi công.\n","sentence  Sơn đảm nhận giữ chức vụ phó giám đốc, có trách nhiệm giúp Giám đốc chỉ đạo và chịu trách nhiệm theo quy định của pháp luật về các lĩnh vực Công tác kế hoạch, chuẩn bị đầu tư và quản lý nhà nước các dự án trọng điểm, dự án đầu tư bằng nguồn vốn trong nước, ký các văn bản liên quan đến công tác quản lý kỹ thuật, khoa học công nghệ môi trường\n","length of inferrence_Task_embedding:  28\n"]}]},{"cell_type":"code","source":["# Tham số threshould: dùng để làm ngưỡng đánh giá các câu là hợp lệ (trong quá trình làm thực tế tự nhận thấy threshold = 0.9 là đạt hiệu quả tốt về chất lượng các câu được cho là đúng)\n","def compare_task(sentence, threshould_ = 0.9):\n","  pre = -1\n","  for task_infer in inferrence_Task_embedding:\n","    ret = calculate_cosine_similarity(task_infer, sentence)\n","    if ret > pre:\n","      pre = ret\n","    if ret > threshould_ :\n","      return ret\n","  return pre"],"metadata":{"id":"gI2t3hL4gd9n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Case 1: Using vnvorenlp** (get name)\n","\n"],"metadata":{"id":"dugEvt30jkDk"}},{"cell_type":"code","source":["!pip install vncorenlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpKKCXn6IPBu","executionInfo":{"status":"ok","timestamp":1692036023829,"user_tz":-420,"elapsed":5839,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"12508601-494a-445c-d367-01ffc6436bb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: vncorenlp in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2023.7.22)\n"]}]},{"cell_type":"code","source":["import os\n","from vncorenlp import VnCoreNLP\n","vncorenlp_folder = '/content/drive/MyDrive/LLM_TEXT/VnCoreNLP-master/'\n","vncorenlp_file = os.path.join(vncorenlp_folder, 'VnCoreNLP-1.1.1.jar')\n","\n","vnlp = VnCoreNLP(vncorenlp_file, annotators=\"wseg,pos,ner\")"],"metadata":{"id":"POtPgcHsITr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"Hôm nay ngày 21 tháng 4 năm 2023 tại trường THCS kim Liên diễn ra cuộcHôm nay, Mai Anh và Hải Đăng đã đến thăm chị Lan tại Hà Nội. họp của ban trung tâm bàn nội \"\n","def getName(str):\n","  annotated_text = vnlp.annotate(str)\n","  person_entities = []\n","  for sentence in annotated_text['sentences']:\n","      for entity in sentence:\n","          if entity[\"nerLabel\"] == 'B-PER':\n","              person_entities.append(entity[\"form\"])\n","  return person_entities\n","\n","# Kiểm tra hàm getName()\n","print(getName(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OUnQ88uVIYrG","executionInfo":{"status":"ok","timestamp":1692036781772,"user_tz":-420,"elapsed":533,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"bb02b967-0602-4166-ef6b-ad85fcbd029e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Mai_Anh', 'Hải_Đăng', 'Lan']\n"]}]},{"cell_type":"markdown","source":["### **Case 2: Pre-train model (PhoBERT)** (get name)"],"metadata":{"id":"XucN2oUfkBtB"}},{"cell_type":"code","source":["import datasets\n","from transformers import RobertaTokenizer, EncoderDecoderModel, AutoTokenizer\n","from sklearn.model_selection import train_test_split\n","\n","#\n","tokenizerName = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n","\n","modelName = EncoderDecoderModel.from_pretrained('/content/drive/MyDrive/LLM_TEXT/trainning_Name/checkpoint-8000')\n","modelName.to(\"cpu\")\n","batch_size = 32  # change to 64 for full evaluation\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dwfhr5rkIcX","executionInfo":{"status":"ok","timestamp":1692034881054,"user_tz":-420,"elapsed":17783,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"c255a156-ad6f-47ef-8934-20ab4d85a9a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","The following encoder weights were not tied to the decoder ['roberta/pooler']\n","The following encoder weights were not tied to the decoder ['roberta/pooler']\n","The following encoder weights were not tied to the decoder ['roberta/pooler']\n"]}]},{"cell_type":"code","source":["# map data correctly\n","def getNameCase2(strInput):\n","    ret = []\n","    inputs = tokenizerName(strInput, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n","    input_ids = inputs.input_ids.to(\"cpu\") #cuda\n","    attention_mask = inputs.attention_mask.to(\"cpu\")\n","    outputs = modelName.generate(input_ids, attention_mask=attention_mask)\n","    output_str = tokenizerName.batch_decode(outputs, skip_special_tokens=True)\n","\n","    # Lọc lại kết quả tên\n","    for name in output_str:\n","      if name in strInput:\n","        ret.append(name)\n","    return ret"],"metadata":{"id":"QBM80ORalSF2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Extract time**"],"metadata":{"id":"SS-39GcG7glR"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","import dateparser\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","# Load AutoTokenizer and AutoModel\n","tokenizer_time = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","model_time = AutoModelForMaskedLM.from_pretrained(\"vinai/phobert-base\")"],"metadata":{"id":"EgyxCpq57nuK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692036639930,"user_tz":-420,"elapsed":2113,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"923a11b3-c3d2-413f-e11f-e24dab4e3652"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["import nltk\n","import re\n","import datefinder\n","\n","import re\n","\n","def extract_month(text):\n","  date_pattern = r\"tháng\\s(\\d{1,2})\"\n","  matches = re.findall(date_pattern, text)\n","  try:\n","    mon = int(matches[0])\n","    return mon\n","  except:\n","    pass\n","  return None\n","\n","#Sử dụng datefinder để trích xuất thời gian\n","def extract_time_datefinder(str_):\n","  # print(\"str_: \", str_)\n","  date_str = {}\n","  matches = []\n","  matches = datefinder.find_dates(str_)\n","  dt = list(matches)\n","  # print(\"dt\", dt)\n","  for time in dt:\n","    day = time.day\n","    month = time.month\n","    year = time.year\n","    month_ = extract_month(str_)\n","    if month_ is not None:\n","       month =  month_\n","    data_ = {\"day\": str(day), \"month\": str(month), \"year\": str(year)}\n","    # date_str.append(data_)\n","    return data_\n","  return None\n","\n","# sử dụng nlp để trích xuất thời gian\n","def extract_time_nlp(text):\n","  tokens = nltk.word_tokenize(text)\n","  time_expressions = [token for token in tokens if nltk.pos_tag([token])[0][1] == \"TIME\"]\n","  if not time_expressions:\n","    return None\n","  return time_expressions[0]\n","\n","#Sử dụng PhoBERT để trích xuất thời gian\n","def extract_time_phobert(text):\n","  input_ids = tokenizer_time.encode(text, add_special_tokens=True)\n","  input_ids = torch.tensor(input_ids).unsqueeze(0)\n","  with torch.no_grad():\n","      outputs = model_time(input_ids)\n","  parsed_date = dateparser.parse(text, languages=[\"vi\"])\n","\n","#Hàm trích xuất thời gian\n","def extract_time(text):\n","  time = extract_time_nlp(text)\n","  if time is not None:\n","    return time\n","\n","  time = extract_time_phobert(text)\n","  if time is not None:\n","    return time\n","\n","  time = extract_time_datefinder(text)\n","  if time is not None:\n","    return time\n","\n","text = \" Deadline ngày 22 tháng 5, 2023\" # Deadline 22 tháng 5, 2023.   ', ' Deadline 22 tháng 5, 2023.  ', ' Deadline 17 tháng 5, 2023.\n","\n","# Extract the time from the text.\n","time1 = extract_time_datefinder(text)\n","\n","# Print the time.\n","print(\"time1: \", time1)\n","# print(\"time1: \", time1[\"day\"])"],"metadata":{"id":"pc7jok0w7uPz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692036790452,"user_tz":-420,"elapsed":321,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"e177fcc4-73bf-4b09-cff8-4702373801f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time1:  {'day': '22', 'month': '5', 'year': '2023'}\n"]}]},{"cell_type":"markdown","source":["### **Summary Task**"],"metadata":{"id":"wX-cqG7Uj434"}},{"cell_type":"code","source":["import nltk\n","from nltk import sent_tokenize\n","# nltk.download('punkt')\n","import re\n","THRESHOLD = 0.9\n","def contains_digits(input_str):\n","    digit_pattern = r'\\d'\n","    match = re.search(digit_pattern, input_str)\n","    return match is not None\n","\n","def process_content_sentences( body: str, min_length=5, max_length=50):\n","        # for word in reversed(stop_words):\n","        #   body.replace(word, \" \")\n","        sents = [s.strip() for s in sent_tokenize(body)]\n","        return [c for c in sents if max_length > len(c.split(\" \")) > min_length]\n","\n","#Hàm trích xuất nhiệm vụ\n","def detectTask(str, case1 = True):\n","  #Phân đoạn văn bản đầu vào\n","  sentences = process_content_sentences(str, 1, 400)\n","  ret = []\n","  for sentence in sentences:\n","    sentencelower = sentence.lower()\n","    time = \"\"\n","\n","    # Trích xuất nhiệm vụ\n","    preci_ = compare_task(sentence, THRESHOLD)\n","    # for wTask in task_words:\n","    #   if wTask.strip() in sentencelower:\n","    if preci_ >= THRESHOLD:\n","      people = []\n","\n","      # Trích xuất tên người\n","      if case1:\n","        people = getName(sentence) # Sử dụng VncoreNLP\n","      else:\n","        people = getNameCase2(sentence) # Sử dụng PhoBERT\n","\n","      ret.append({\"people\": people, \"content\": sentencelower, \"time\": {'day': -1 , 'month': -1, 'year': -1} })\n","\n","    # Trích xuất thời gian\n","    time_ =  extract_time(sentence)\n","    if time_ is not None:\n","      daystr = \"\"\n","      try:\n","        if len(ret) > 0:\n","          daystr = time_[\"day\"]\n","          if daystr.strip() in sentencelower:\n","            ret[-1][\"time\"][\"day\"] = time_[\"day\"]\n","            ret[-1][\"time\"][\"month\"] = time_[\"month\"]\n","            ret[-1][\"time\"][\"year\"] = time_[\"year\"]\n","      except:\n","        print(\" errr\")\n","        pass\n","\n","  return ret\n","\n","def detectTaskeyword(str, case1 = True):\n","  sentences = process_content_sentences(str, 1, 400)\n","  ret = []\n","  for sentence in sentences:\n","    sentencelower = sentence.lower()\n","    time = \"\"\n","\n","    # Trích xuất nhiệm vụ sử dụng keyword\n","    for wTask in task_words:\n","      if wTask.strip() in sentencelower:\n","        people = []\n","        if case1:\n","          people = getName(sentence)\n","        else:\n","          people = getNameCase2(sentence)\n","\n","        ret.append({\"people\": people, \"content\": sentencelower, \"time\": {'day': -1 , 'month': -1, 'year': -1} })\n","        break\n","    time_ =  extract_time(sentence)\n","    if time_ is not None:\n","      daystr = \"\"\n","      try:\n","        if len(ret) > 0:\n","          daystr = time_[\"day\"]\n","          if daystr.strip() in sentencelower:\n","            ret[-1][\"time\"][\"day\"] = time_[\"day\"]\n","            ret[-1][\"time\"][\"month\"] = time_[\"month\"]\n","            ret[-1][\"time\"][\"year\"] = time_[\"year\"]\n","      except:\n","        print(\" errr\")\n","        pass\n","\n","  return ret"],"metadata":{"id":"yuc-Otjl0rh4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **summary content**"],"metadata":{"id":"lXTgitx_yNgi"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Using `max_length`'s default\")\n"],"metadata":{"id":"5b2mRnd_F0pP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","from transformers import RobertaTokenizer, EncoderDecoderModel, AutoTokenizer\n","from sklearn.model_selection import train_test_split\n","\n","# load model\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n","\n","model = EncoderDecoderModel.from_pretrained('/content/drive/MyDrive/LLM_TEXT/trainning_summary_content_bbch/checkpoint-8000')\n","model.to(\"cpu\")\n","batch_size = 32  # change to 64 for full evaluation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ahgf2SNyrpW","executionInfo":{"status":"ok","timestamp":1692036822958,"user_tz":-420,"elapsed":6432,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"38a1653c-2587-4a2a-f4de-3cc8faa7e55d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","The following encoder weights were not tied to the decoder ['roberta/pooler']\n","The following encoder weights were not tied to the decoder ['roberta/pooler']\n","The following encoder weights were not tied to the decoder ['roberta/pooler']\n"]}]},{"cell_type":"code","source":["# tóm tắt nội dung sử dụng PhoBERT\n","def generate_summary(strOriginal):\n","    # Tokenizer will automatically set [BOS] <text> [EOS]\n","    inputs = tokenizer(strOriginal, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n","    input_ids = inputs.input_ids.to(\"cpu\")\n","    attention_mask = inputs.attention_mask.to(\"cpu\")\n","    outputs = model.generate(input_ids, attention_mask=attention_mask)\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","    return output_str"],"metadata":{"id":"80WQHCOxIroI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Demo**"],"metadata":{"id":"_8AK_JfRBcjn"}},{"cell_type":"code","source":["def inference(inputText):\n","  print(\"Văn bản gốc: \", inputText)\n","  cntTask = 0\n","  summaryContent = generate_summary(inputText)\n","  print(\"\\nNội dung cuộc họp được tóm tắt: \", summaryContent)\n","\n","  #Trích xuất nhiệm vụ Sử dụng cosine_similarity\n","  print(\"\\nTrích xuất nhiệm vụ sử dụng cosine_similarity\")\n","  ret_task = detectTask(inputText, False)\n","  for task in ret_task:\n","    print(\"Nhiệm vụ số \", cntTask)\n","    print(\"Người làm: \", task['people'])\n","    print(\"thời gian: \", task['time'])\n","    print(\"Nội dung nhiệm vụ: \", task['content'])\n","    print (\"\")\n","    cntTask += 1\n","\n","  cntTask = 0\n","  # Trích xuất nhiệm vụ sử dụng keyword\n","  print(\"\\nTrích xuất nhiệm vụ sử dụng keyword\")\n","  ret_task = detectTaskeyword(inputText, False)\n","  for task in ret_task:\n","    print(\"Nhiệm vụ số \", cntTask)\n","    print(\"Người làm: \", task['people'])\n","    print(\"thời gian: \", task['time'])\n","    print(\"Nội dung nhiệm vụ: \", task['content'])\n","    print (\"\")\n","    cntTask += 1"],"metadata":{"id":"NcW_wRd21l7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_demo_01 = '''Hôm nay ngày 21 tháng 9 năm 2017 ban chỉ đạo trường THPT Hoài Đức A họp bàn về việc phân công trách nhiệm cho từng thành viên trong Ban chỉ đạo thực hiện công tác phòng chống bệnh sốt xuất huyết.\n","Ban chỉ đạo gồm có 5 thành viên có tên sau : Nga, Linh, Hùng , Hạnh , Nguyên. Nga hiệu trưởng cũng chính là trưởng ban chỉ đạo phòng chống dịch bệnh thông qua nội dung thực hiện phòng bệnh và giao nhiệm vụ cho từng thành viên. Sau đây là nhiệm vụ cụ thể của từng người: Linh sẽ đảm nhận trách nhiệm hướng dẫn CBGVNV cùng nắm chắc các phương pháp phòng chống bệnh sốt xuất huyết đồng thời chú trọng công tác an toàn thực phẩm,\n"," kiểm tra vệ sinh, khử khuẩn phòng chống dịch tại trường. Có kế hoạch kiểm tra, giám sát việc thực hiện công tác của từng thành viên trong trường. Deadline 24 tháng 9, 2017.\n"," Hùng đảm nhận phụ trách công tác y tế, tham mưu, giúp đỡ và cùng phối hợp với nhà trường trong công tác phòng chống bệnh sốt xuất huyết, tổ chức các buổi tuyên truyền về công tác phòng chống bệnh. Deadline 25 tháng 9, 2017.\n"," Hạnh phối kết hợp cùng các đồng chí trong Ban chỉ đạo theo dõi, giám sát việc thực hiện công tác bệnh sốt xuất huyết. Nguyên đảm nhận tham gia kiểm tra, theo dõi, giám sát việc thực hiện công tác phòng chống bệnh sốt xuất huyết. Thời gian chúng ta thực hiện bắt đầu từ hôm nay. Cuộc họp kết thúc với sự quyết tâm chống dịch cao của các thành viên.'''"],"metadata":{"id":"OZRA3Mww9qVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inference(text_demo_01)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MgrvOVyLBdn","executionInfo":{"status":"ok","timestamp":1692036903369,"user_tz":-420,"elapsed":61517,"user":{"displayName":"Viết Trường Nguyễn","userId":"03804394364627562790"}},"outputId":"0c74b177-bf1e-44e4-e7aa-93ada3cfad1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Văn bản gốc:  Hôm nay ngày 21 tháng 9 năm 2017 ban chỉ đạo trường THPT Hoài Đức A họp bàn về việc phân công trách nhiệm cho từng thành viên trong Ban chỉ đạo thực hiện công tác phòng chống bệnh sốt xuất huyết. \n","Ban chỉ đạo gồm có 5 thành viên có tên sau : Nga, Linh, Hùng , Hạnh , Nguyên. Nga hiệu trưởng cũng chính là trưởng ban chỉ đạo phòng chống dịch bệnh thông qua nội dung thực hiện phòng bệnh và giao nhiệm vụ cho từng thành viên. Sau đây là nhiệm vụ cụ thể của từng người: Linh sẽ đảm nhận trách nhiệm hướng dẫn CBGVNV cùng nắm chắc các phương pháp phòng chống bệnh sốt xuất huyết đồng thời chú trọng công tác an toàn thực phẩm,\n"," kiểm tra vệ sinh, khử khuẩn phòng chống dịch tại trường. Có kế hoạch kiểm tra, giám sát việc thực hiện công tác của từng thành viên trong trường. Deadline 24 tháng 9, 2017. \n"," Hùng đảm nhận phụ trách công tác y tế, tham mưu, giúp đỡ và cùng phối hợp với nhà trường trong công tác phòng chống bệnh sốt xuất huyết, tổ chức các buổi tuyên truyền về công tác phòng chống bệnh. Deadline 25 tháng 9, 2017. \n"," Hạnh phối kết hợp cùng các đồng chí trong Ban chỉ đạo theo dõi, giám sát việc thực hiện công tác bệnh sốt xuất huyết. Nguyên đảm nhận tham gia kiểm tra, theo dõi, giám sát việc thực hiện công tác phòng chống bệnh sốt xuất huyết. Thời gian chúng ta thực hiện bắt đầu từ hôm nay. Cuộc họp kết thúc với sự quyết tâm chống dịch cao của các thành viên.\n","\n","Nội dung cuộc họp được tóm tắt:  ['Sau chỉ đạo trường THCS Hồng Thái họp bàn về việc phân công trách nhiệm cho từng thành viên trong Ban chỉ đạo thực hiện công tác phòng chống bệnh']\n","\n","Trích xuất nhiệm vụ sử dụng cosine_similarity\n","Nhiệm vụ số  0\n","Người làm:  ['Linh']\n","thời gian:  {'day': '24', 'month': '9', 'year': '2023'}\n","Nội dung nhiệm vụ:  sau đây là nhiệm vụ cụ thể của từng người: linh sẽ đảm nhận trách nhiệm hướng dẫn cbgvnv cùng nắm chắc các phương pháp phòng chống bệnh sốt xuất huyết đồng thời chú trọng công tác an toàn thực phẩm,\n"," kiểm tra vệ sinh, khử khuẩn phòng chống dịch tại trường.\n","\n","Nhiệm vụ số  1\n","Người làm:  ['Hùng']\n","thời gian:  {'day': '25', 'month': '9', 'year': '2023'}\n","Nội dung nhiệm vụ:  hùng đảm nhận phụ trách công tác y tế, tham mưu, giúp đỡ và cùng phối hợp với nhà trường trong công tác phòng chống bệnh sốt xuất huyết, tổ chức các buổi tuyên truyền về công tác phòng chống bệnh.\n","\n","\n","Trích xuất nhiệm vụ sử dụng keyword\n","Nhiệm vụ số  0\n","Người làm:  ['Nga']\n","thời gian:  {'day': -1, 'month': -1, 'year': -1}\n","Nội dung nhiệm vụ:  nga hiệu trưởng cũng chính là trưởng ban chỉ đạo phòng chống dịch bệnh thông qua nội dung thực hiện phòng bệnh và giao nhiệm vụ cho từng thành viên.\n","\n","Nhiệm vụ số  1\n","Người làm:  ['Linh']\n","thời gian:  {'day': '24', 'month': '9', 'year': '2023'}\n","Nội dung nhiệm vụ:  sau đây là nhiệm vụ cụ thể của từng người: linh sẽ đảm nhận trách nhiệm hướng dẫn cbgvnv cùng nắm chắc các phương pháp phòng chống bệnh sốt xuất huyết đồng thời chú trọng công tác an toàn thực phẩm,\n"," kiểm tra vệ sinh, khử khuẩn phòng chống dịch tại trường.\n","\n","Nhiệm vụ số  2\n","Người làm:  ['Hùng']\n","thời gian:  {'day': '25', 'month': '9', 'year': '2023'}\n","Nội dung nhiệm vụ:  hùng đảm nhận phụ trách công tác y tế, tham mưu, giúp đỡ và cùng phối hợp với nhà trường trong công tác phòng chống bệnh sốt xuất huyết, tổ chức các buổi tuyên truyền về công tác phòng chống bệnh.\n","\n","Nhiệm vụ số  3\n","Người làm:  ['Nguyên']\n","thời gian:  {'day': -1, 'month': -1, 'year': -1}\n","Nội dung nhiệm vụ:  nguyên đảm nhận tham gia kiểm tra, theo dõi, giám sát việc thực hiện công tác phòng chống bệnh sốt xuất huyết.\n","\n"]}]}]}